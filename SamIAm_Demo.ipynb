{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyML0R5LxO8/n9TKCEEFFaOR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PerfLab-EXaCT/SamIAm-LabelStudio/blob/main/SamIAm_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4kKJgyf9Up2Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NzXhv-Z4IM0",
        "outputId": "fdaefbc1-0161-4675-de5a-45d5c56dfb95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SamIAm'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 47 (delta 18), reused 42 (delta 13), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (47/47), 53.75 KiB | 5.38 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/PerfLab-EXaCT/SamIAm.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b9nQLvi4QsG",
        "outputId": "1a538758-e235-4452-8969-87e9528fc064"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-04 21:15:58--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.165.83.79, 18.165.83.44, 18.165.83.91, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.165.83.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2564550879 (2.4G) [binary/octet-stream]\n",
            "Saving to: ‘sam_vit_h_4b8939.pth’\n",
            "\n",
            "sam_vit_h_4b8939.pt 100%[===================>]   2.39G   135MB/s    in 22s     \n",
            "\n",
            "2024-03-04 21:16:19 (113 MB/s) - ‘sam_vit_h_4b8939.pth’ saved [2564550879/2564550879]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r SamIAm/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj7qVIh04V6y",
        "outputId": "f53e9e23-2c6b-475e-bbc2-43112952dcff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/segment-anything.git (from -r SamIAm/requirements.txt (line 1))\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-3jms93tz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-3jms93tz\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/huggingface/transformers.git (from -r SamIAm/requirements.txt (line 2))\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-y484syoi\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-y484syoi\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 836921fdeb498820b71dcc7b70e990e828f4c6bc\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datasets (from -r SamIAm/requirements.txt (line 3))\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting monai (from -r SamIAm/requirements.txt (line 4))\n",
            "  Downloading monai-1.3.0-202310121228-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting patchify (from -r SamIAm/requirements.txt (line 5))\n",
            "  Downloading patchify-0.2.3-py3-none-any.whl (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0->-r SamIAm/requirements.txt (line 2)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0->-r SamIAm/requirements.txt (line 2)) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0->-r SamIAm/requirements.txt (line 2)) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0->-r SamIAm/requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0->-r SamIAm/requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0->-r SamIAm/requirements.txt (line 2)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0->-r SamIAm/requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0->-r SamIAm/requirements.txt (line 2)) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0->-r SamIAm/requirements.txt (line 2)) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0->-r SamIAm/requirements.txt (line 2)) (4.66.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r SamIAm/requirements.txt (line 3)) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->-r SamIAm/requirements.txt (line 3)) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r SamIAm/requirements.txt (line 3))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r SamIAm/requirements.txt (line 3)) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r SamIAm/requirements.txt (line 3)) (3.4.1)\n",
            "Collecting multiprocess (from datasets->-r SamIAm/requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r SamIAm/requirements.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r SamIAm/requirements.txt (line 3)) (3.9.3)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from monai->-r SamIAm/requirements.txt (line 4)) (2.1.0+cu121)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r SamIAm/requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r SamIAm/requirements.txt (line 3)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r SamIAm/requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r SamIAm/requirements.txt (line 3)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r SamIAm/requirements.txt (line 3)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r SamIAm/requirements.txt (line 3)) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0->-r SamIAm/requirements.txt (line 2)) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0->-r SamIAm/requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0->-r SamIAm/requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0->-r SamIAm/requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0->-r SamIAm/requirements.txt (line 2)) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai->-r SamIAm/requirements.txt (line 4)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai->-r SamIAm/requirements.txt (line 4)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai->-r SamIAm/requirements.txt (line 4)) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai->-r SamIAm/requirements.txt (line 4)) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r SamIAm/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r SamIAm/requirements.txt (line 3)) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->-r SamIAm/requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->monai->-r SamIAm/requirements.txt (line 4)) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->monai->-r SamIAm/requirements.txt (line 4)) (1.3.0)\n",
            "Building wheels for collected packages: segment-anything, transformers\n",
            "  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36587 sha256=eb69478e8dc15b872058b16e8b96a1f1ac0bea7d0a55ce35b5c5244fe8494813\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-24kfry25/wheels/10/cf/59/9ccb2f0a1bcc81d4fbd0e501680b5d088d690c6cfbc02dc99d\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.39.0.dev0-py3-none-any.whl size=8648192 sha256=f72e679cdee9d2bfcdeaf12420143218d9429842180b7ba3bf9b390babc384c0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-24kfry25/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
            "Successfully built segment-anything transformers\n",
            "Installing collected packages: segment-anything, patchify, dill, multiprocess, monai, transformers, datasets\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.38.1\n",
            "    Uninstalling transformers-4.38.1:\n",
            "      Successfully uninstalled transformers-4.38.1\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 monai-1.3.0 multiprocess-0.70.16 patchify-0.2.3 segment-anything-1.0 transformers-4.39.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ninja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SplS_bSa45e6",
        "outputId": "9d0d7ea1-6965-41b8-b7db-852c50f5dc9c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/307.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m297.0/307.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-encoding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E92jCGFh4-dI",
        "outputId": "8dcca16f-4dfa-4e2d-c1db-5d25dc760cb2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-encoding\n",
            "  Downloading torch_encoding-1.2.1-py2.py3-none-any.whl (126 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/126.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.2/126.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-encoding) (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-encoding) (4.66.2)\n",
            "Collecting nose (from torch-encoding)\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from torch-encoding)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from torch-encoding) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from torch-encoding) (0.16.0+cu121)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from torch-encoding) (9.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-encoding) (1.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-encoding) (2.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torch-encoding) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torch-encoding) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torch-encoding) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torch-encoding) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torch-encoding) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torch-encoding) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->torch-encoding) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-encoding) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-encoding) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-encoding) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-encoding) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->torch-encoding) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->torch-encoding) (1.3.0)\n",
            "Installing collected packages: nose, portalocker, torch-encoding\n",
            "Successfully installed nose-1.3.7 portalocker-2.8.2 torch-encoding-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segment-anything"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhVCBWWS7aDQ",
        "outputId": "24c93d27-b797-4ace-b017-0267284d6b27"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: segment-anything in /usr/local/lib/python3.10/dist-packages (1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://gitlab.com/perflab-exact/chess/chess-data.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZlYdSvDY1Y2",
        "outputId": "8e7110e6-71cb-488c-93ea-5e19df887ff3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chess-data'...\n",
            "remote: Enumerating objects: 2382, done.\u001b[K\n",
            "remote: Counting objects: 100% (2382/2382), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2005/2005), done.\u001b[K\n",
            "remote: Total 2382 (delta 379), reused 2302 (delta 372), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2382/2382), 125.37 MiB | 22.50 MiB/s, done.\n",
            "Resolving deltas: 100% (379/379), done.\n",
            "Updating files: 100% (2334/2334), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, Subset, DataLoader\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import torch.quantization\n",
        "from sklearn.cluster import DBSCAN, KMeans\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.decomposition import PCA\n",
        "import random\n",
        "import sys\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "from datetime import datetime\n",
        "sys.path.append(\"SamIAm\")\n",
        "from utils import *"
      ],
      "metadata": {
        "id": "vU-8rW9N4cxL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChessDataset(Dataset):\n",
        "    def __init__(self, folder_path, transform=None):\n",
        "        self.image_paths = glob.glob(os.path.join(folder_path, '*.tiff')) + glob.glob(os.path.join(folder_path, '*.jpg'))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = self.image_paths[index]\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Extract the filename from the image path\n",
        "        filename = os.path.basename(image_path)\n",
        "\n",
        "        return image, filename\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "NOW = str(datetime.now()).replace(\" \",\"--\").split(\".\")[0]\n",
        "\n",
        "#Define default global variable values\n",
        "SAM = 'sam_vit_h_4b8939.pth'\n",
        "BATCH_SIZE = 1\n",
        "SUBSET_SIZE = 1\n",
        "IDEAL_CHIP_SIZE = 60\n",
        "DATA_DIR = 'chess-data/stem-data/dataset1/images/' # Expert 1\n",
        "OUTPUT_DIR = '/logs/' + NOW + '/'\n",
        "COPY_ORIGINAL_DATA = True\n",
        "OVERLAP = 90\n",
        "K = 10\n",
        "\n",
        "#SamAutoMaskGenerator params\n",
        "POINTS_PER_SIDE = 12\n",
        "CROP_N_LAYERS = 1\n",
        "PRED_IOU_THRESH = 0.9\n",
        "STABILITY_SCORE_THRESH = 0.92\n",
        "CROP_N_POINTS_DOWNSCALE_FACTOR = 2"
      ],
      "metadata": {
        "id": "TL_e_5WLVbvp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ARGUMENT():\n",
        "    pass\n",
        "args = ARGUMENT()\n",
        "args.sam = \"sam_vit_h_4b8939.pth\"\n",
        "args.grid = 12\n",
        "args.chipsize = 60\n",
        "args.dilation = 1\n",
        "args.iou_thresh = 0\n",
        "args.model = \"FENet\"\n",
        "args.post = 1\n",
        "args.embed = 1\n",
        "args.par = 0\n",
        "args.saveformat = \"png\"\n",
        "\n",
        "SAM = args.sam\n",
        "POINTS_PER_SIDE = args.grid\n",
        "IDEAL_CHIP_SIZE = args.chipsize\n",
        "DILATION = args.dilation\n",
        "IOU_THRESH = args.iou_thresh\n",
        "\n",
        "OPTIONS = {\n",
        "'SAM': SAM,\n",
        "'Image batch size': BATCH_SIZE,\n",
        "'Experiment data subset size': SUBSET_SIZE,\n",
        "'Ideal chip size': IDEAL_CHIP_SIZE,\n",
        "'Data dir': DATA_DIR,\n",
        "'Chip mask overlap percentage': OVERLAP,\n",
        "'Chip sample size (K)': K,\n",
        "'Chip classifier/encoder': args.model,\n",
        "'Embed chips (0 or 1)': args.embed,\n",
        "'Post processing (0 or 1)': args.post,\n",
        "'POINTS_PER_SIDE': POINTS_PER_SIDE,\n",
        "'CROP_N_LAYERS': CROP_N_LAYERS,\n",
        "'PRED_IOU_THRESH': PRED_IOU_THRESH,\n",
        "'STABILITY_SCORE_THRESH': STABILITY_SCORE_THRESH,\n",
        "'CROP_N_POINTS_DOWNSCALE_FACTOR': CROP_N_POINTS_DOWNSCALE_FACTOR,\n",
        "'BOUNDARY MASK DILATION':DILATION,\n",
        "'IOU_THRESH for pairing mask and label': IOU_THRESH,\n",
        "'Par computation (0 or 1)': args.par,\n",
        "}\n",
        "\n",
        "os.makedirs(os.path.dirname(os.path.abspath(os.getcwd()) + OUTPUT_DIR), exist_ok=True)\n",
        "\n",
        "#Write OPTIONS to file\n",
        "titles = [\"OPTIONS\", \"VALUES\"]\n",
        "spacing = [40,20]\n",
        "write_to_file(OPTIONS, titles, os.path.abspath(os.getcwd()) + OUTPUT_DIR + 'options.log',spacing)\n",
        "\n",
        "logger = get_logger()\n",
        "\n",
        "#sys.path.append(\".\")\n",
        "sam_checkpoint = SAM\n",
        "if 'vit_h' in SAM:\n",
        "    model_type = \"vit_h\"\n",
        "elif 'vit_l' in SAM:\n",
        "    model_type = \"vit_l\"\n",
        "elif 'vit_b' in SAM:\n",
        "    model_type = \"vit_b\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"DEVICE:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6Hw9IWwt82_",
        "outputId": "0b04dbdb-dfd3-48fc-8281-a9bcc4e7aeac"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "startTime = datetime.now()\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "print(\"SAM\",device,\"model loading time:\", datetime.now() - startTime)\n",
        "\n",
        "# Define the transformation(s) you want to apply to the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Add more transformations as needed\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "XjYQAdcPYQM-",
        "outputId": "ee4d34cd-a31b-472f-f243-3d9e88f5f8fd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAM cpu model loading time: 0:00:16.191227\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ChessDataset' object has no attribute 'size'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-d43da8b251e9>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Instantiate the custom dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChessDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# Slice the dataset to get the first X images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0msubset_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSUBSET_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ChessDataset' object has no attribute 'size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(DATA_DIR)\n",
        "# Instantiate the custom dataset\n",
        "dataset = ChessDataset(DATA_DIR, transform=transform)\n",
        "print(len(dataset))\n",
        "# Slice the dataset to get the first X images\n",
        "subset_dataset = Subset(dataset, indices=range(SUBSET_SIZE))\n",
        "\n",
        "# Instantiate the DataLoader with the subset dataset\n",
        "dataloader = DataLoader(subset_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "#Instantiate num materials for each material\n",
        "#Expert 1:\n",
        "num_materials = {\n",
        "                \"STEM_ADF_11-02-18_10_nm_LFO-STO_A_LO_0_103118_0002\":3,\n",
        "                \"STEM_JEOL_HAADF_04-27-17_13_nm_STO_p-Ge_033117_LO_110_042617_HAADF_0001\":4,\n",
        "                \"STEM_JEOL_ADF1_02-20-20_Yingge_4nm_WO3_-_NbSTO_052617_LO_020620_0005\":3,\n",
        "                \"STEM_JEOL_ADF1_10-12-20_La0-8Sr0-2FeO3-STO-080317-2-LO-zero-deg_0004_1\":4,\n",
        "                \"STEM_JEOL_ADF1_02-20-20_Yingge_4nm_WO3_-_NbSTO_052617_LO_020620_0001\":3,\n",
        "                \"STEM_ADF_11-02-18_10_nm_LFO-STO_A_LO_0_103118_0001\":3,\n",
        "                \"STEM_JEOL_ADF1_02-20-20_Yingge_4nm_WO3_-_NbSTO_052617_LO_020620_0002\":3,\n",
        "                \"STEM_ADF_09-24-18_30_nm_LMO_STO_081317B_LO_091618_0004\":3,\n",
        "                \"STEM_JEOL_ADF1_03-16-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_0005_1\":3,\n",
        "                \"STEM_JEOL_ADF1_03-16-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_0001\":4,\n",
        "                \"STEM_JEOL_ADF1_10-12-20_La0-8Sr0-2FeO3-STO-080317-2-LO-zero-deg_0001_1\":4,\n",
        "                \"STEM_JEOL_ADF1_10-12-20_La0-8Sr0-2FeO3-STO-080317-2-LO-zero-deg_0005_1\":3,\n",
        "                \"STEM_ADF_11-02-18_10_nm_LFO-STO_A_LO_0_103118_0004\":4,\n",
        "                \"STEM_JEOL_ADF1_03-16-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_0003\":4,\n",
        "                \"STEM_JEOL_ADF1_06-29-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_Thinner_0004\":3,\n",
        "                \"STEM_JEOL_ADF1_03-16-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_0002\":4,\n",
        "                \"STEM_JEOL_ADF1_03-16-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_0006\":2,\n",
        "                \"STEM_JEOL_ADF1_06-29-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_Thinner_0002\":4,\n",
        "                \"STEM_ADF_09-24-18_30_nm_LMO_STO_081317B_LO_091618_0006\":3,\n",
        "                \"STEM_ADF_09-24-18_30_nm_LMO_STO_081317B_LO_091618_0008\":3,\n",
        "                \"STEM_JEOL_ADF1_10-12-20_La0-8Sr0-2FeO3-STO-080317-2-LO-zero-deg_0002\":4,\n",
        "                \"STEM_JEOL_ADF1_06-29-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_Thinner_0003_1\":4,\n",
        "                \"STEM_ADF_09-24-18_30_nm_LMO_STO_081317B_LO_091618_0005\":3,\n",
        "                \"STEM_ADF_11-02-18_10_nm_LFO-STO_A_LO_0_103118_0003\":3,\n",
        "                \"STEM_JEOL_HAADF_04-27-17_13_nm_STO_p-Ge_033117_LO_110_042617_HAADF_0005\":3,\n",
        "                \"STEM_JEOL_ADF1_03-16-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_0004_1\":3,\n",
        "                \"STEM_JEOL_HAADF_04-27-17_13_nm_STO_p-Ge_033117_LO_110_042617_HAADF_0006\":3,\n",
        "                \"STEM_JEOL_HAADF_04-27-17_13_nm_STO_p-Ge_033117_LO_110_042617_HAADF_0003\":4,\n",
        "                \"STEM_JEOL_ADF1_10-12-20_La0-8Sr0-2FeO3-STO-080317-2-LO-zero-deg_0003_1\":4,\n",
        "                \"STEM_JEOL_ADF1_10-12-20_La0-8Sr0-2FeO3-STO-080317-2-LO-zero-deg_0006_1\":2,\n",
        "                \"STEM_JEOL_HAADF_04-27-17_13_nm_STO_p-Ge_033117_LO_110_042617_HAADF_0007\":3,\n",
        "                \"STEM_JEOL_ADF1_06-29-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_Thinner_0001\":4,\n",
        "                }\n",
        "\n",
        "mixin_coeef = {\n",
        "    \"STEM_ADF_11-02-18_10_nm_LFO-STO_A_LO_0_103118_0002\":{},\n",
        "    \"STEM_JEOL_HAADF_04-27-17_13_nm_STO_p-Ge_033117_LO_110_042617_HAADF_0001\":{'task-12-annotation-20-by-2-tag-Ge-10.png':10,'task-12-annotation-20-by-2-tag-PtC-10.png':10,'task-12-annotation-20-by-2-tag-SrTiO3-1.png':1,'task-12-annotation-20-by-2-tag-Vac-10.png':10},\n",
        "    \"STEM_JEOL_ADF1_02-20-20_Yingge_4nm_WO3_-_NbSTO_052617_LO_020620_0005\":{},\n",
        "    \"STEM_JEOL_ADF1_10-12-20_La0-8Sr0-2FeO3-STO-080317-2-LO-zero-deg_0004_1\":{'SrTiO3-merged-2.png':10,'task-30-annotation-34-by-2-tag-LSFO1-1.png':1,'task-30-annotation-34-by-2-tag-LSFO2-1.png':1,'task-30-annotation-34-by-2-tag-PtC-2.png':10},\n",
        "    \"STEM_JEOL_ADF1_02-20-20_Yingge_4nm_WO3_-_NbSTO_052617_LO_020620_0001\":{},\n",
        "    \"STEM_ADF_11-02-18_10_nm_LFO-STO_A_LO_0_103118_0001\":{},\n",
        "    \"STEM_JEOL_ADF1_02-20-20_Yingge_4nm_WO3_-_NbSTO_052617_LO_020620_0002\":{'task-10-annotation-14-by-2-tag-PtC-10.png':10,'task-10-annotation-14-by-2-tag-SrTiO3-10.png':10,'task-10-annotation-14-by-2-tag-WO3-1.png':1},\n",
        "    \"STEM_ADF_09-24-18_30_nm_LMO_STO_081317B_LO_091618_0004\":{},\n",
        "    \"STEM_JEOL_ADF1_03-16-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_0005_1\":{'La-merged-2.png':10,'task-25-annotation-29-by-2-tag-PtC-1.png':1,'task-25-annotation-29-by-2-tag-Unk-1.png':1},\n",
        "    \"STEM_JEOL_ADF1_03-16-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_0001\":{'PtC-merged-1.png':1,'task-21-annotation-26-by-2-tag-La-SrTiO3-1.png':1,'task-21-annotation-26-by-2-tag-SrTiO3-2.png':10,'task-21-annotation-26-by-2-tag-Vac-2.png':10},\n",
        "    \"STEM_JEOL_ADF1_10-12-20_La0-8Sr0-2FeO3-STO-080317-2-LO-zero-deg_0001_1\":{},\n",
        "    \"STEM_JEOL_ADF1_10-12-20_La0-8Sr0-2FeO3-STO-080317-2-LO-zero-deg_0005_1\":{'task-31-annotation-35-by-2-tag-LSFO1-1.png':1,'task-31-annotation-35-by-2-tag-LSFO2-1.png':1,'task-31-annotation-35-by-2-tag-SrTiO3-2.png':10},\n",
        "    \"STEM_ADF_11-02-18_10_nm_LFO-STO_A_LO_0_103118_0004\":{'task-4-annotation-8-by-2-tag-LaFeO3-1.png':1,'task-4-annotation-8-by-2-tag-PtC-10.png':10,'task-4-annotation-8-by-2-tag-SrTiO3-10.png':10,'task-4-annotation-8-by-2-tag-Vac-10.png':10},\n",
        "    \"STEM_JEOL_ADF1_03-16-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_0003\":{'task-23-annotation-27-by-2-tag-La-SrTiO3-1.png':1,'task-23-annotation-27-by-2-tag-PtC-10.png':10,'task-23-annotation-27-by-2-tag-SrTiO3-10.png':10,'task-23-annotation-27-by-2-tag-Vac-10.png':10},\n",
        "    \"STEM_JEOL_ADF1_06-29-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_Thinner_0004\":{},\n",
        "    \"STEM_JEOL_ADF1_03-16-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_0002\":{'task-22-annotation-25-by-2-tag-La-SrTiO3-1.png':1,'task-22-annotation-25-by-2-tag-PtC-10.png':10,'task-22-annotation-25-by-2-tag-SrTiO3-10.png':10,'task-22-annotation-25-by-2-tag-Vac-10.png':10},\n",
        "    \"STEM_JEOL_ADF1_03-16-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_0006\":{},\n",
        "    \"STEM_JEOL_ADF1_06-29-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_Thinner_0002\":{},\n",
        "    \"STEM_ADF_09-24-18_30_nm_LMO_STO_081317B_LO_091618_0006\":{},\n",
        "    \"STEM_ADF_09-24-18_30_nm_LMO_STO_081317B_LO_091618_0008\":{},\n",
        "    \"STEM_JEOL_ADF1_10-12-20_La0-8Sr0-2FeO3-STO-080317-2-LO-zero-deg_0002\":{},\n",
        "    \"STEM_JEOL_ADF1_06-29-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_Thinner_0003_1\":{'task-19-annotation-23-by-2-tag-La-SrTiO3-10.png':10,'task-19-annotation-23-by-2-tag-PtC-10.png':10,'task-19-annotation-23-by-2-tag-SrTiO3-10.png':10,'task-19-annotation-23-by-2-tag-Unk-1.png':1},\n",
        "    \"STEM_ADF_09-24-18_30_nm_LMO_STO_081317B_LO_091618_0005\":{},\n",
        "    \"STEM_ADF_11-02-18_10_nm_LFO-STO_A_LO_0_103118_0003\":{},\n",
        "    \"STEM_JEOL_HAADF_04-27-17_13_nm_STO_p-Ge_033117_LO_110_042617_HAADF_0005\":{},\n",
        "    \"STEM_JEOL_ADF1_03-16-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_0004_1\":{'task-24-annotation-28-by-2-tag-La-SrTiO3-10.png':10,'task-24-annotation-28-by-2-tag-PtC-10.png':10,'task-24-annotation-28-by-2-tag-SrTiO3-1.png':1},\n",
        "    \"STEM_JEOL_HAADF_04-27-17_13_nm_STO_p-Ge_033117_LO_110_042617_HAADF_0006\":{},\n",
        "    \"STEM_JEOL_HAADF_04-27-17_13_nm_STO_p-Ge_033117_LO_110_042617_HAADF_0003\":{},\n",
        "    \"STEM_JEOL_ADF1_10-12-20_La0-8Sr0-2FeO3-STO-080317-2-LO-zero-deg_0003_1\":{'LSFO1-merged-1.png':1,'task-29-annotation-33-by-2-tag-LSFO2-1.png':1,'task-29-annotation-33-by-2-tag-PtC-2.png':10,'task-29-annotation-33-by-2-tag-SrTiO3-2.png':10},\n",
        "    \"STEM_JEOL_ADF1_10-12-20_La0-8Sr0-2FeO3-STO-080317-2-LO-zero-deg_0006_1\":{'LSFO1-merged-1.png':1,'task-32-annotation-36-by-2-tag-LSFO2-1.png':1},\n",
        "    \"STEM_JEOL_HAADF_04-27-17_13_nm_STO_p-Ge_033117_LO_110_042617_HAADF_0007\":{},\n",
        "    \"STEM_JEOL_ADF1_06-29-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_Thinner_0001\":{'PtC-merged-2.png':10,'task-17-annotation-21-by-2-tag-La-SrTiO3-1.png':1,'task-17-annotation-21-by-2-tag-SrTiO3-2.png':10,'task-17-annotation-21-by-2-tag-Vac-2.png':10},\n",
        "}\n",
        "\n",
        "# #Expert 2:\n",
        "\n",
        "# num_materials = {\n",
        "#     \"STEM_ADF_09-24-18-50-nm-LMO-STO-081618-LO-091618_0002\":4,\n",
        "#     \"STEM_JEOL-ADF1_12-10-2020-LaFeO3-STO-092917-b-LO-zero-deg-12-8-2020_0112_1\":0,\n",
        "#     \"STEM_JEOL-ADF1_11-20-19-Spurgeon-60-nm-LaMnO3-STO-001-073119-LO-103019_0007\":0,\n",
        "#     \"STEM_ADF_09-24-18-50-nm-LMO-STO-081618-LO-091618_0008\":2,\n",
        "#     \"STEM_JEOL-ADF1_12-10-2020-LaFeO3-STO-092917-b-LO-zero-deg-12-8-2020_0109_1\":0,\n",
        "#     \"STEM_JEOL-ADF1_03-16-20-Wangoh-LSTO-STO-0.25-TEM-012020-LO-0-031020_0001\":2,\n",
        "#     \"STEM_JEOL-ADF1_03-16-20-Wangoh-LSTO-STO-0.25-TEM-012020-LO-0-031020_0005_1\":3,\n",
        "#     \"STEM_JEOL-ADF1_11-20-19-Spurgeon-60-nm-LaMnO3-STO-001-073119-LO-103019_0011\":0,\n",
        "#     \"STEM_JEOL-ADF1_11-20-19-Spurgeon-60-nm-LaMnO3-STO-001-073119-LO-103019_0015\":0,\n",
        "#     \"STEM_JEOL-ADF1_02-20-20-Yingge-4nm-WO3-NbSTO-052617-LO-020620_0002\":3,\n",
        "#     \"STEM_JEOL-HAADF_04-27-17-13-nm-STO-p-Ge-033117-LO-110-042617-HAADF_0006\":3,\n",
        "#     \"STEM_JEOL-ADF1_07-23-19-Du-STO-Ge-LO-45-071919_0007\":0,\n",
        "#     \"STEM_ADF_11-02-18-10-nm-LFO-STO-A-LO-0-103118_0003\":3,\n",
        "#     \"STEM_JEOL-ADF1_12-10-2020-LaFeO3-STO-092917-b-LO-zero-deg-12-8-2020_0115_1\":0,\n",
        "#     \"STEM_ADF_09-24-18-50-nm-LMO-STO-081618-LO-091618_0004\":3,\n",
        "#     \"STEM_JEOL-ADF1_08-13-20-Wang-1-1-STO-SNO-LSAT-062620-a-LO-45-081320_0010_1\":0,\n",
        "#     \"STEM_JEOL-ADF1_03-16-20-Wangoh-LSTO-STO-0.25-TEM-012020-LO-0-031020_0006\":2,\n",
        "#     \"STEM_JEOL-ADF1_12-10-2020-LaFeO3-STO-092917-b-LO-zero-deg-12-8-2020_0106_1\":0,\n",
        "#     \"STEM_JEOL-ADF1_07-23-19-Du-STO-Ge-LO-45-071919_0010\":0,\n",
        "#     \"STEM_JEOL-ADF1_08-13-20-Wang-1-1-STO-SNO-LSAT-062620-a-LO-45-081320_0006_1\":0,\n",
        "#     \"STEM_JEOL-ADF1_12-10-2020-LaFeO3-STO-092917-b-LO-zero-deg-12-8-2020_0117_1\":0,\n",
        "#     \"STEM_JEOL-ADF1_12-10-2020-LaFeO3-STO-092917-b-LO-zero-deg-12-8-2020_0118_1\":0,\n",
        "#     \"STEM_JEOL-ADF1_06-29-20-Wangoh-LSTO-STO-0.25-TEM-012020-LO-0-031020-Thinner_0003_1\":3,\n",
        "#     \"STEM_JEOL-ADF1_08-03-20-Wang-1-STO-1-SNO-LSAT-062620-a-LO-45-072720_0001\":0,\n",
        "#     \"STEM_JEOL-ADF1_11-20-19-Spurgeon-60-nm-LaMnO3-STO-001-073119-LO-103019_0013\":0,\n",
        "#     \"STEM_ADF_11-02-18-10-nm-LFO-STO-A-LO-0-103118_0002\":3,\n",
        "#     \"STEM_JEOL-ADF1_08-13-20-Wang-1-1-STO-SNO-LSAT-062620-a-LO-45-081320_0003\":0,\n",
        "#     \"STEM_JEOL-ADF1_08-13-20-Wang-1-1-STO-SNO-LSAT-062620-a-LO-45-081320_0012\":0,\n",
        "#     \"STEM_JEOL-ADF1_08-13-20-Wang-1-1-STO-SNO-LSAT-062620-a-LO-45-081320_0005_1\":0,\n",
        "#     \"STEM_JEOL-ADF1_06-29-20-Wangoh-LSTO-STO-0.25-TEM-012020-LO-0-031020-Thinner_0004\":3,\n",
        "#     \"STEM_JEOL-ADF1_11-20-19-Spurgeon-60-nm-LaMnO3-STO-001-073119-LO-103019_0009\":0,\n",
        "#     \"STEM_JEOL-ADF1_07-23-19-Du-STO-Ge-LO-45-071919_0012\":0,\n",
        "#     \"STEM_JEOL-ADF1_02-20-20-Yingge-4nm-WO3-NbSTO-052617-LO-020620_0005\":3,\n",
        "#     \"STEM_JEOL-ADF1_11-20-19-Spurgeon-60-nm-LaMnO3-STO-001-073119-LO-103019_0005\":0,\n",
        "#     \"STEM_JEOL-ADF1_08-13-20-Wang-1-1-STO-SNO-LSAT-062620-a-LO-45-081320_0008\":0,\n",
        "#     \"STEM_JEOL-HAADF_05-02-17-5-uc-STO-p-Ge-033117-LO-110-051116-HAADF_0007\":0,\n",
        "#     \"STEM_JEOL-ADF1_12-10-2020-LaFeO3-STO-092917-b-LO-zero-deg-12-8-2020_0105\":0,\n",
        "#     \"STEM_JEOL-ADF1_10-12-20-La0.8Sr0.2FeO3-STO-080317-2-LO-zero-deg_0005_1\":2,\n",
        "#     \"STEM_ADF_09-24-18-50-nm-LMO-STO-081618-LO-091618_0009\":2,\n",
        "#     \"STEM_JEOL-HAADF_05-02-17-5-uc-STO-p-Ge-033117-LO-110-051116-HAADF_0003\":0,\n",
        "#     \"STEM_ADF_01-12-18-4-uc-LaMnO3-072817A-LO-100-01118-EELS_0004\":3,\n",
        "#     \"STEM_ADF_09-24-18-50-nm-LMO-STO-081618-LO-091618_0007\":2,\n",
        "#     \"STEM_JEOL-ADF1_07-23-19-Du-STO-Ge-LO-45-071919_0001\":0,\n",
        "#     \"STEM_JEOL-ADF1_03-16-20-Wangoh-LSTO-STO-0.25-TEM-012020-LO-0-031020_0004_1\":3,\n",
        "#     \"STEM_JEOL-ADF1_02-20-20-Yingge-4nm-WO3-NbSTO-052617-LO-020620_0001\":3,\n",
        "#     \"STEM_JEOL-HAADF_05-02-17-5-uc-STO-p-Ge-033117-LO-110-051116-HAADF_0005\":0,\n",
        "#     \"STEM_JEOL-HAADF_04-27-17-13-nm-STO-p-Ge-033117-LO-110-042617-HAADF_0001\":3,\n",
        "#     \"STEM_ADF_09-24-18-30-nm-LMO-STO-081317B-LO-091618_0006\":3,\n",
        "#     \"STEM_JEOL-HAADF_05-02-17-5-uc-STO-p-Ge-033117-LO-110-051116-HAADF_0001\":0,\n",
        "#     \"STEM_JEOL-HAADF_05-02-17-5-uc-STO-p-Ge-033117-LO-110-051116-HAADF_0002\":0,\n",
        "#     \"STEM_JEOL-HAADF_04-27-17-13-nm-STO-p-Ge-033117-LO-110-042617-HAADF_0003\":3,\n",
        "#     \"STEM_JEOL-ADF1_10-12-20-La0.8Sr0.2FeO3-STO-080317-2-LO-zero-deg_0004_1\":3,\n",
        "#     \"STEM_JEOL-ADF1_08-13-20-Wang-1-1-STO-SNO-LSAT-062620-a-LO-45-081320_0002\":0,\n",
        "#     \"STEM_JEOL-ADF1_12-10-2020-LaFeO3-STO-092917-b-LO-zero-deg-12-8-2020_0119_1\":0,\n",
        "#     \"STEM_JEOL-ADF1_08-13-20-Wang-1-1-STO-SNO-LSAT-062620-a-LO-45-081320_0011\":0,\n",
        "#     \"STEM_ADF_09-24-18-50-nm-LMO-STO-081618-LO-091618_0001\":4,\n",
        "#     \"STEM_ADF_01-15-18-2-uc-LaMnO3-070617B-LO-100-111417-EELS_0004\":3,\n",
        "#     \"STEM_JEOL-ADF1_03-16-20-Wangoh-LSTO-STO-0.25-TEM-012020-LO-0-031020_0002\":4,\n",
        "#     \"STEM_JEOL-ADF1_06-29-20-Wangoh-LSTO-STO-0.25-TEM-012020-LO-0-031020-Thinner_0002\":4,\n",
        "#     \"STEM_ADF_09-24-18-50-nm-LMO-STO-081618-LO-091618_0006\":3,\n",
        "#     \"STEM_JEOL-ADF1_08-03-20-Wang-1-STO-1-SNO-LSAT-062620-a-LO-45-072720_0002\":0,\n",
        "#     \"STEM_JEOL-ADF1_12-10-2020-LaFeO3-STO-092917-b-LO-zero-deg-12-8-2020_0114_1\":0,\n",
        "#     \"STEM_JEOL-HAADF_04-27-17-13-nm-STO-p-Ge-033117-LO-110-042617-HAADF_0005\":3,\n",
        "#     \"STEM_ADF_09-24-18-50-nm-LMO-STO-081618-LO-091618_0003\":3,\n",
        "#     \"STEM_JEOL-ADF1_03-16-20-Wangoh-LSTO-STO-0.25-TEM-012020-LO-0-031020_0003\":4,\n",
        "#     \"STEM_ADF_01-15-18-2-uc-LaMnO3-070617B-LO-100-111417-EELS_0003\":3,\n",
        "#     \"STEM_JEOL-ADF1_08-03-20-Wang-1-STO-1-SNO-LSAT-062620-a-LO-45-072720_0003\":0,\n",
        "#     \"STEM_JEOL-ADF1_08-13-20-Wang-1-1-STO-SNO-LSAT-062620-a-LO-45-081320_0007_1\":0,\n",
        "#     \"STEM_ADF_09-24-18-50-nm-LMO-STO-081618-LO-091618_0005\":3,\n",
        "#     \"STEM_ADF_01-12-18-4-uc-LaMnO3-072817A-LO-100-01118-EELS_0006\":3,\n",
        "#     \"STEM_JEOL-ADF1_12-10-2020-LaFeO3-STO-092917-b-LO-zero-deg-12-8-2020_0107_1\":0,\n",
        "#     \"STEM_JEOL-ADF1_12-10-2020-LaFeO3-STO-092917-b-LO-zero-deg-12-8-2020_0116_1\":0,\n",
        "#     \"STEM_JEOL-ADF1_08-13-20-Wang-1-1-STO-SNO-LSAT-062620-a-LO-45-081320_0001\":0,\n",
        "#     \"STEM_JEOL-ADF1_07-23-19-Du-STO-Ge-LO-45-071919_0003\":0,\n",
        "#     \"STEM_JEOL-ADF1_12-10-2020-LaFeO3-STO-092917-b-LO-zero-deg-12-8-2020_0108_1\":0,\n",
        "#     \"STEM_JEOL-HAADF_05-02-17-5-uc-STO-p-Ge-033117-LO-110-051116-HAADF_0006\":0,\n",
        "#     \"STEM_ADF_01-15-18-2-uc-LaMnO3-070617B-LO-100-111417-EELS_0002\":3,\n",
        "#     \"STEM_JEOL-ADF1_07-23-19-Du-STO-Ge-LO-45-071919_0005\":0,\n",
        "#     \"STEM_JEOL-ADF1_08-13-20-Wang-1-1-STO-SNO-LSAT-062620-a-LO-45-081320_0004-2\":0,\n",
        "#     \"STEM_ADF_09-24-18-30-nm-LMO-STO-081317B-LO-091618_0004\":3,\n",
        "#     \"STEM_JEOL-ADF1_11-20-19-Spurgeon-60-nm-LaMnO3-STO-001-073119-LO-103019_0001\":0,\n",
        "#     \"STEM_ADF_01-12-18-4-uc-LaMnO3-072817A-LO-100-01118-EELS_0005\":3,\n",
        "#     \"STEM_ADF_01-12-18-4-uc-LaMnO3-072817A-LO-100-01118-EELS_0003\":3,\n",
        "#     \"STEM_JEOL-ADF1_12-10-2020-LaFeO3-STO-092917-b-LO-zero-deg-12-8-2020_0110_1\":0,\n",
        "#     \"STEM_JEOL-ADF1_10-12-20-La0.8Sr0.2FeO3-STO-080317-2-LO-zero-deg_0002\":4,\n",
        "#     \"STEM_JEOL-ADF1_06-29-20-Wangoh-LSTO-STO-0.25-TEM-012020-LO-0-031020-Thinner_0001\":4,\n",
        "#     \"STEM_ADF_11-02-18-10-nm-LFO-STO-A-LO-0-103118_0001\":3,\n",
        "#     \"STEM_JEOL-ADF1_10-12-20-La0.8Sr0.2FeO3-STO-080317-2-LO-zero-deg_0001_1\":4,\n",
        "#     \"STEM_JEOL-ADF1_10-12-20-La0.8Sr0.2FeO3-STO-080317-2-LO-zero-deg_0006_1\":2,\n",
        "#     \"STEM_ADF_09-24-18-30-nm-LMO-STO-081317B-LO-091618_0005\":3,\n",
        "#     \"STEM_JEOL-ADF1_12-10-2020-LaFeO3-STO-092917-b-LO-zero-deg-12-8-2020_0120_1\":0,\n",
        "#     \"STEM_JEOL-ADF1_08-13-20-Wang-1-1-STO-SNO-LSAT-062620-a-LO-45-081320_0009\":0,\n",
        "#     \"STEM_JEOL-ADF1_12-10-2020-LaFeO3-STO-092917-b-LO-zero-deg-12-8-2020_0111_1\":0,\n",
        "#     \"STEM_JEOL-ADF1_10-12-20-La0.8Sr0.2FeO3-STO-080317-2-LO-zero-deg_0003_1\":3,\n",
        "#     \"STEM_ADF_11-02-18-10-nm-LFO-STO-A-LO-0-103118_0004\":3,\n",
        "#     \"STEM_JEOL-ADF1_12-10-2020-LaFeO3-STO-092917-b-LO-zero-deg-12-8-2020_0121_1\":0,\n",
        "#     \"STEM_JEOL-ADF1_08-13-20-Wang-1-1-STO-SNO-LSAT-062620-a-LO-45-081320_0004_1\":0,\n",
        "#     \"STEM_JEOL-ADF1_11-20-19-Spurgeon-60-nm-LaMnO3-STO-001-073119-LO-103019_0003\":0,\n",
        "#     \"STEM_ADF_09-24-18-30-nm-LMO-STO-081317B-LO-091618_0008\":3,\n",
        "#     \"STEM_ADF_01-15-18-2-uc-LaMnO3-070617B-LO-100-111417-EELS_0001\":3,\n",
        "#     \"STEM_ADF_01-12-18-4-uc-LaMnO3-072817A-LO-100-01118-EELS_0007\":3,\n",
        "#     \"STEM_JEOL-HAADF_04-27-17-13-nm-STO-p-Ge-033117-LO-110-042617-HAADF_0007\":3,\n",
        "#     \"STEM_JEOL-ADF1_08-03-20-Wang-1-STO-1-SNO-LSAT-062620-a-LO-45-072720_0004_1\":0,\n",
        "# }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO1P_x9OamC1",
        "outputId": "a4040616-0db5-494a-c6f6-649366a29bf7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chess-data/stem-data/dataset1/images/\n",
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Collect all performance measures\n",
        "#key : IMAGE_ALIAS, values : [iou, recall, precision, f1, fpr]\n",
        "results = {}\n",
        "\n",
        "# Iterate over the dataloader to access the images and labels\n",
        "for image, filename in dataloader:\n",
        "\n",
        "    print(\"processing:\",filename)\n",
        "    _, _, width, height = image.shape\n",
        "    ROWS, COLS = int(height / IDEAL_CHIP_SIZE), int(width / IDEAL_CHIP_SIZE)\n",
        "\n",
        "    IMAGE_NAME = filename[0]\n",
        "    IMAGE_PATH = DATA_DIR + IMAGE_NAME\n",
        "    IMAGE_ALIAS = IMAGE_NAME\n",
        "    if '.' in IMAGE_ALIAS:\n",
        "        IMAGE_ALIAS = IMAGE_ALIAS.split('.')[0]\n",
        "    LABEL_PATH =  'chess-data/stem-data/dataset1/labels/' + IMAGE_ALIAS   # Expert 1\n",
        "\n",
        "    #Initialize reulsts as none for this image\n",
        "    results[IMAGE_ALIAS] = [None, None, None, None, None]\n",
        "\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print('IMAGE:',IMAGE_NAME)\n",
        "    image_name, image_ext = os.path.splitext(IMAGE_NAME)\n",
        "    CLUSTERS = num_materials[image_name] if image_name in num_materials.keys() else 3\n",
        "    print(f'{image_name}: clusters {\"\" if image_name in num_materials.keys() else \"not\"} found!')\n",
        "\n",
        "    #Prepare logfile\n",
        "    log_dir = os.path.abspath(os.getcwd()) + OUTPUT_DIR + IMAGE_ALIAS + '/'\n",
        "    log_file_name = 'output'\n",
        "    logger = init_logs(log_file_name, log_dir=log_dir)\n",
        "\n",
        "    #Copy image from dataset to output file\n",
        "    if COPY_ORIGINAL_DATA:\n",
        "        source_file = DATA_DIR + filename[0]\n",
        "        shutil.copy(source_file, log_dir)\n",
        "\n",
        "\n",
        "    startTime = datetime.now()\n",
        "    CHIPS, squares = chip_image(IMAGE_PATH,ROWS, COLS)\n",
        "    logger.info(f'Image chipping: {datetime.now() - startTime}')\n",
        "\n",
        "    CHIP_SIZE = squares[0][3]\n",
        "\n",
        "    #print(squares)\n",
        "\n",
        "    image = cv2.imread(IMAGE_PATH)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    logger.info(f'{IMAGE_ALIAS}-->{image.shape} = {ROWS} x {COLS} \"\\tchip_size: {CHIP_SIZE}')\n",
        "\n",
        "    startTime = datetime.now()\n",
        "    #mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "\n",
        "    mask_generator = SamAutomaticMaskGenerator(\n",
        "        model=sam,\n",
        "        points_per_side=POINTS_PER_SIDE,\n",
        "        pred_iou_thresh=PRED_IOU_THRESH,\n",
        "        stability_score_thresh=STABILITY_SCORE_THRESH,\n",
        "        crop_n_layers=CROP_N_LAYERS,\n",
        "        crop_n_points_downscale_factor=CROP_N_POINTS_DOWNSCALE_FACTOR,\n",
        "        min_mask_region_area=IDEAL_CHIP_SIZE ** 2,  # Requires open-cv to run post-processing\n",
        "    )\n",
        "\n",
        "    masks = mask_generator.generate(image)\n",
        "    logger.info(f'SAM {device} mask generation time: {datetime.now() - startTime}')\n",
        "\n",
        "    #Add 'status' key to all mask dictionaries\n",
        "    for mask in masks:\n",
        "        mask[\"status\"] = \"ok\"\n",
        "\n",
        "    #Post-processing mask filtering\n",
        "    #Remove tiny component masks inside a compund mask if sum of compnent masks < 50% of compund mask\n",
        "    #Remove compund mask if component masks makeup >= 70% of compund mask\n",
        "    if args.post:\n",
        "        startTime = datetime.now()\n",
        "        mask_tree = {i:{\"children\":[], \"redundant\":[]} for i, _ in enumerate(masks)}\n",
        "        for i, mask_1 in enumerate(masks):\n",
        "            for j, mask_2 in enumerate(masks):\n",
        "                if i != j:\n",
        "                    mask_2_overlap, mask_1_overlap, intersection_overlap = mutual_overlap(mask_1[\"segmentation\"], mask_2[\"segmentation\"])\n",
        "                    #mask2 is child/component of mask1\n",
        "                    if mask_2_overlap > 90 and mask_1_overlap < 90:\n",
        "                        mask_tree[i][\"children\"].append([j,intersection_overlap])\n",
        "                    #mask2 and mask1 are highly overlapping (redundant)\n",
        "                    elif mask_2_overlap > 90 and mask_1_overlap > 90:\n",
        "                        mask_tree[i][\"redundant\"].append([j,intersection_overlap])\n",
        "            logger.info(f'MASK-{i} \\t Children: {mask_tree[i][\"children\"]}  \\t Redundant: {mask_tree[i][\"redundant\"]}')\n",
        "\n",
        "        #Remove unnecessary masks\n",
        "        expendables = set()\n",
        "        #Remove child if it's less than 5% of parent\n",
        "        for i,v in mask_tree.items():\n",
        "            for j,v in v[\"children\"]:\n",
        "                if v < 5:\n",
        "                    expendables.add(j)\n",
        "                    masks[j][\"status\"] = \"tiny\"\n",
        "\n",
        "        #Remove parent if > 70% is madeup of children\n",
        "        for i,v in mask_tree.items():\n",
        "            covered = 0\n",
        "            for j,v in v[\"children\"]:\n",
        "                if v > 5:\n",
        "                    covered += v\n",
        "            if covered > 70:\n",
        "                expendables.add(i)\n",
        "                masks[i][\"status\"] = \"compound\"\n",
        "\n",
        "        #Remove the samller of redundant masks\n",
        "        for i,v in mask_tree.items():\n",
        "            for j,v in v[\"redundant\"]:\n",
        "                j_degree = v #Degree to which mask_j overlaps/belongs to mask_i\n",
        "                for a,b in mask_tree[j][\"redundant\"]:\n",
        "                    if a == i:\n",
        "                        i_degree = b #Degree to which mask_i overlaps/belongs to with mask_j\n",
        "                expendables.add(i if j_degree > i_degree else j)\n",
        "                masks[i if j_degree > i_degree else j][\"status\"] = \"redundant\"\n",
        "\n",
        "        logger.info(f'Expendables: {expendables}')\n",
        "\n",
        "        masks = [mask for i, mask in enumerate(masks) if i not in expendables]\n",
        "\n",
        "        logger.info(f\"Post-processing time: {datetime.now() - startTime}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    startTime = datetime.now()\n",
        "    #Compute all valid chips for the image\n",
        "    for square in squares:\n",
        "        for idx,mask in enumerate(masks):\n",
        "            if mask[\"status\"] == \"ok\":\n",
        "                overlap = chip_mask_overlap(square,mask[\"segmentation\"])\n",
        "                if overlap >= OVERLAP:\n",
        "                    logger.info(f\"Chip: {square}  x  Mask-{idx}: {mask['bbox']}   -->  {overlap:.2f}% overlap\")\n",
        "                    #Chips are in form (name,row,column,width,length). Convert to (name,x0,y0,x1,y1) for overlaying.\n",
        "                    chip = (square[0],square[1],square[2],square[1] + square[3],square[2] + square[4])\n",
        "                    if \"chips\" in mask.keys():\n",
        "                        mask[\"chips\"].append(chip)\n",
        "                    else:\n",
        "                        mask[\"chips\"] = [chip]\n",
        "    logger.info(f'Mask chip matching: {datetime.now() - startTime}')\n",
        "\n",
        "    startTime = datetime.now()\n",
        "    #Select k: where k is the sample chip size for each mask\n",
        "    k = K\n",
        "    for idx,mask in enumerate(masks):\n",
        "        if mask[\"status\"] == \"ok\":\n",
        "            if 'chips' not in mask.keys():\n",
        "                mask[\"chips\"] = []\n",
        "            k_chips = []\n",
        "            if len(mask[\"chips\"]) > k:\n",
        "                k_chips = random.shuffle(mask[\"chips\"])\n",
        "                k_chips = mask[\"chips\"][:k]\n",
        "            elif len(mask[\"chips\"]) > 0 and len(mask[\"chips\"]) < k:\n",
        "                k_chips = mask[\"chips\"]\n",
        "            mask[\"k_chips\"] = k_chips\n",
        "            #mask[\"k_chips\"] = mask[\"chips\"]\n",
        "            logger.info(f\"Mask-{idx}: {mask['bbox']}, area {mask['area']}   -->  {len(mask['k_chips'])} chips\")\n",
        "    logger.info(f'Select k chips per mask: {datetime.now() - startTime}')\n",
        "\n",
        "\n",
        "    startTime = datetime.now()\n",
        "    #Cut out the k chips for each mask and save them to disk\n",
        "    for idx,mask in enumerate(masks):\n",
        "        if mask[\"status\"] == \"ok\":\n",
        "            #Chips are saved as (name,x0,y0,x1,y1) for overlaying. Convert back to to (name,row,column,width,length) for chipping.\n",
        "            k_chips = {square[0] : CHIPS[square[0]] for square in mask[\"k_chips\"]}\n",
        "            save_chips(IMAGE_PATH,f\"MASK-{idx}\",k_chips,savepath=OUTPUT_DIR,imgs_ext='.jpg')\n",
        "    logger.info(f'Save k-masks to disk: {datetime.now() - startTime}')\n",
        "\n",
        "    #If no embedding needed, instantiate model in order to compute soft-labels\n",
        "    if not args.embed:\n",
        "        if args.model == \"res18\":\n",
        "            # For ResNet18 chip classifier\n",
        "            model = models.resnet18(pretrained=True)\n",
        "            num_classes = 64\n",
        "            model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "            model.load_state_dict(torch.load('res18.pth'))\n",
        "\n",
        "        elif args.model == \"FENet\":\n",
        "            # For FENet chip classifier\n",
        "            opt = None\n",
        "            with open('FENet_DTD_opt.pth', 'rb') as file:\n",
        "                opt = pickle.load(file)\n",
        "            model = FENet18(opt)\n",
        "            model.load_state_dict(torch.load('FENet_DTD.pth'))\n",
        "\n",
        "    startTime = datetime.now()\n",
        "    #Encode masks\n",
        "    encodings = []\n",
        "    centroids = []\n",
        "    encoding_dim = 64\n",
        "    for idx,mask in enumerate(masks):\n",
        "        #images_path = \"./k_chips/\" + IMAGE_PATH.split(\".\")[-2].split(\"/\")[-1] + f\"/MASK-{idx}\"\n",
        "        images_path = log_dir + f\"MASK-{idx}\"\n",
        "        if args.embed:\n",
        "            mask[\"encodings\"] = encode_chips(images_path,args.model,layer_index=4,weights='IMAGENET1K_V1') #'IMAGENET1K_V1',\"res18.pth\"\n",
        "            if mask[\"encodings\"]:\n",
        "                mask[\"centroid\"] = np.mean(mask[\"encodings\"], axis=0)\n",
        "                encodings += mask[\"encodings\"]\n",
        "                centroids.append(mask[\"centroid\"])\n",
        "            logger.info(f\"Mask-{idx} \\t encoded: {len(mask['encodings'])}\")\n",
        "        else:\n",
        "            mask[\"soft_preds\"] = soft_predict_chips(images_path,model)\n",
        "            #Random pred augmentation/repetition for smaller masks\n",
        "            if mask[\"soft_preds\"]:\n",
        "                while len(mask[\"soft_preds\"]) < k:\n",
        "                    random_pred = random.choice(mask[\"soft_preds\"])\n",
        "                    mask[\"soft_preds\"].append(random_pred)\n",
        "            logger.info(f\"Mask-{idx} \\t preds: {len(mask['soft_preds'])}\")\n",
        "    logger.info(f\"{'Encode' if args.embed else 'Soft-pred'} k-masks: {datetime.now() - startTime}\")\n",
        "\n",
        "    n_materials = CLUSTERS #len(masks)\n",
        "    kmeans = KMeans(n_clusters=n_materials)\n",
        "    unknown_color = np.concatenate([np.random.random(3), [0.35]])\n",
        "    color_code = {'unknown':unknown_color}\n",
        "\n",
        "    if args.post and args.embed:\n",
        "        #Cluster masks based on encodings\n",
        "        startTime = datetime.now()\n",
        "        gmm = GaussianMixture(n_components=CLUSTERS)\n",
        "        dbscan = DBSCAN(eps=0.5, min_samples=2)\n",
        "        #labels_train = dbscan.fit_predict(np.array(encodings))\n",
        "        labels_train = kmeans.fit(np.array(encodings)).labels_\n",
        "        #gmm.fit(np.array(encodings))\n",
        "        left, right = 0, 0\n",
        "\n",
        "        #logger.info(f\"Labels: {labels}\")\n",
        "        #Encode masks\n",
        "        color_code = {}\n",
        "        label_masks = {label:[] for label in labels_train}\n",
        "        for idx,mask in enumerate(masks):\n",
        "            mask[\"color\"] = unknown_color\n",
        "            if mask[\"encodings\"]:\n",
        "                # print(\"Pred\",kmeans.predict(mask[\"encodings\"]))\n",
        "                labels = list(kmeans.predict(np.array(mask[\"encodings\"])))\n",
        "                # right = left + len(mask[\"encodings\"])\n",
        "                # labels = list(labels_train[left:right])\n",
        "                # left = right\n",
        "                #labels = list(gmm.predict(np.array(mask[\"encodings\"])))\n",
        "                mask[\"label\"] = max(set(labels), key=lambda x: labels.count(x))\n",
        "                label_masks[mask['label']].append(idx)\n",
        "                if not mask[\"label\"] in color_code.keys():\n",
        "                    color_code[mask[\"label\"]] = np.concatenate([np.random.random(3), [0.35]])\n",
        "\n",
        "                mask[\"color\"] = color_code[mask[\"label\"]]\n",
        "                logger.info(f\"Mask-{idx} \\t max: {mask['label']} \\t labels: {labels}\")\n",
        "\n",
        "        logger.info(f'Labeling time: {datetime.now() - startTime}')\n",
        "        # Perform dimensionality reduction using PCA\n",
        "        pca = PCA(n_components=2)\n",
        "        reduced_vectors = pca.fit_transform(np.array(encodings))\n",
        "\n",
        "        # Plot the points with colors indicating labels\n",
        "        plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], c=labels_train, cmap='viridis')\n",
        "        plt.colorbar()\n",
        "\n",
        "        # Add labels and title to the plot\n",
        "        plt.xlabel('PC1')\n",
        "        plt.ylabel('PC2')\n",
        "        plt.title('KMeans Clustering')\n",
        "        encoding_out = log_dir + '_encodings.pdf' if args.saveformat == 'pdf' else log_dir + '_encodings.png'\n",
        "        plt.draw()\n",
        "        plt.savefig(encoding_out, bbox_inches='tight', pad_inches=0, format='pdf' if args.saveformat == 'pdf' else 'png')\n",
        "        plt.close()\n",
        "\n",
        "    elif args.post and not args.embed:\n",
        "        startTime = datetime.now()\n",
        "        kl_adj = {i:[] for i in range(len(masks))}\n",
        "        for i in range(len(masks)):\n",
        "            mask_i = masks[i]\n",
        "            if mask[\"status\"] == \"ok\" and mask_i[\"soft_preds\"] and len(mask_i[\"soft_preds\"]) == k:\n",
        "                for j in range(len(masks)):\n",
        "                    mask_j = masks[j]\n",
        "                    if mask_j[\"soft_preds\"] and len(mask_j[\"soft_preds\"]) == k:\n",
        "                        #logger.info(f\"(Mask-{i}, Mask-{j}) \\t = {torch.stack(mask_i['soft_preds'])} \\t {torch.stack(mask_j['soft_preds'])}\")\n",
        "                        if args.method == 'kl':\n",
        "                            kl_div = kl(mask_i[\"soft_preds\"],mask_j[\"soft_preds\"])\n",
        "                        elif args.method == 'emd':\n",
        "                            kl_div = emd(mask_i[\"soft_preds\"],mask_j[\"soft_preds\"])\n",
        "                        #logger.info(f\"KL-div(Mask-{i}, Mask-{j}) \\t = {kl_div}\")\n",
        "                        kl_adj[i].append(kl_div)\n",
        "                    else:\n",
        "                        kl_adj[i].append(-1)\n",
        "\n",
        "        logger.info(f'KL computation time: {datetime.now() - startTime}')\n",
        "\n",
        "        data,names,weights = [],[],[]\n",
        "        for i,v in kl_adj.items():\n",
        "            if v:\n",
        "                data.append(v)\n",
        "                names.append(f'Mask-{i}')\n",
        "                weights.append(len(masks[i]['k_chips']))\n",
        "                #logger.info(f\"Mask-{i} => chips {len(masks[i]['k_chips'])}\")\n",
        "\n",
        "        if len(data) <  2:\n",
        "            continue\n",
        "\n",
        "        #Redefine kmeans n_components\n",
        "        n_materials = min(CLUSTERS,len(data)) if CLUSTERS else min(3,len(data))\n",
        "        kmeans = KMeans(n_clusters=n_materials)\n",
        "\n",
        "        # Perform dimensionality reduction using PCA\n",
        "        pca = PCA(n_components=2)\n",
        "        reduced_vectors = pca.fit_transform(np.array(data))\n",
        "\n",
        "        labels = kmeans.fit(reduced_vectors).labels_\n",
        "        centers = kmeans.cluster_centers_\n",
        "\n",
        "        # silhouette_values = silhouette_samples(reduced_vectors, labels)\n",
        "        # #silhouette_avg = silhouette_score(reduced_vectors, labels)\n",
        "        # weighted_silhouette_avg = np.average(silhouette_values, weights=weights)\n",
        "\n",
        "        # logger.info(f\"Weighted Silhouette score: {weighted_silhouette_avg}\")\n",
        "\n",
        "        unknown_color = np.concatenate([np.random.random(3), [0.35]])\n",
        "        unknown_mask_idxs = []\n",
        "        idx = 0\n",
        "        label_masks = {label:[] for label in labels}\n",
        "        for i,v in kl_adj.items():\n",
        "            if v:\n",
        "                masks[i][\"label\"] = labels[idx]\n",
        "                idx += 1\n",
        "                label_masks[masks[i]['label']].append(i)\n",
        "            else:\n",
        "                masks[i][\"label\"] = -1\n",
        "\n",
        "            if masks[i][\"label\"] == -1:\n",
        "                masks[i][\"color\"] = unknown_color\n",
        "                unknown_mask_idxs.append(i)\n",
        "            elif masks[i][\"label\"] not in color_code.keys():\n",
        "                color_code[masks[i][\"label\"]] = np.concatenate([np.random.random(3), [0.35]])\n",
        "                masks[i][\"color\"] = color_code[masks[i][\"label\"]]\n",
        "            else:\n",
        "                masks[i][\"color\"] = color_code[masks[i][\"label\"]]\n",
        "\n",
        "            logger.info(f\"Mask-{i} \\t label: {masks[i]['label']} \\t vector = {v}\")\n",
        "\n",
        "        #Filter unknown mask indices\n",
        "        #masks = [value for index, value in enumerate(masks) if index not in unknown_mask_idxs]\n",
        "\n",
        "\n",
        "        # Perform dimensionality reduction using PCA\n",
        "        # pca = PCA(n_components=2)\n",
        "        # reduced_vectors = pca.fit_transform(np.array(data))\n",
        "\n",
        "\n",
        "        # Generate meshgrid to plot cluster regions\n",
        "        step_size = 0.01\n",
        "        x_min, x_max = reduced_vectors[:, 0].min() - 0.1, reduced_vectors[:, 0].max() + 0.1\n",
        "        y_min, y_max = reduced_vectors[:, 1].min() - 0.1, reduced_vectors[:, 1].max() + 0.1\n",
        "        xx, yy = np.meshgrid(np.arange(x_min, x_max, step_size), np.arange(y_min, y_max, step_size))\n",
        "        Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "\n",
        "        # Plot the points with colors indicating labels\n",
        "        plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], c=labels, cmap='viridis')\n",
        "        plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
        "        plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='x', s=100)\n",
        "        plt.colorbar()\n",
        "\n",
        "        for i, txt in enumerate(names):\n",
        "            plt.annotate(txt, (reduced_vectors[i][0], reduced_vectors[i][1]),fontsize=8)\n",
        "\n",
        "        # Add labels and title to the plot\n",
        "        plt.xlabel('PC1')\n",
        "        plt.ylabel('PC2')\n",
        "        plt.title('KMeans Clustering')\n",
        "        encoding_out = log_dir + '_encodings.pdf' if args.saveformat == 'pdf' else log_dir + '_encodings.png'\n",
        "        plt.draw()\n",
        "        plt.savefig(encoding_out, bbox_inches='tight', pad_inches=0, format='pdf' if args.saveformat == 'pdf' else 'png')\n",
        "        plt.close()\n",
        "\n",
        "    #Post-processing merge masks with similar labels\n",
        "    if args.post and len(masks) > CLUSTERS:\n",
        "        expendables, merged, merged_labels = [], [], []\n",
        "        for label,mask_ids in label_masks.items():\n",
        "            if len(mask_ids) > 1:\n",
        "                expendables += mask_ids\n",
        "                submasks = [masks[i] for i in mask_ids]\n",
        "                merged.append(merge_masks(submasks))\n",
        "                merged_labels.append(label)\n",
        "                for mask in submasks:\n",
        "                    mask[\"status\"] = \"component\"\n",
        "\n",
        "        #masks = [mask for i, mask in enumerate(masks) if i not in expendables]\n",
        "        #labels = [label for i, label in enumerate(labels) if i not in expendables]\n",
        "        masks += merged\n",
        "        labels = np.append(labels,merged_labels)\n",
        "    elif len(masks) <= CLUSTERS:\n",
        "        print(f'Avoided merging since masks: {len(masks)} <= clusters: {CLUSTERS}')\n",
        "\n",
        "    #Performance measure\n",
        "    file_list = [file for file in os.listdir(LABEL_PATH) if file.endswith(\".png\")]\n",
        "\n",
        "    num_gt_labels = len(file_list)\n",
        "\n",
        "    # Iterate through the list of .png files and load them as PIL Image objects\n",
        "    avg_iou, avg_pre, avg_rec, avg_f1, total_fpr = 0, 0, 0, 0, 0\n",
        "    for file_name in file_list:\n",
        "        file_path = os.path.join(LABEL_PATH, file_name)\n",
        "        ground_truth_mask = Image.open(file_path)\n",
        "        # Convert the PIL image to a NumPy array\n",
        "        mask_array = np.array(ground_truth_mask)\n",
        "\n",
        "        if len(mask_array.shape) > 2:\n",
        "            mask_array = mask_array[:,:,1]\n",
        "\n",
        "        img_area = mask_array.shape[0] * mask_array.shape[1]\n",
        "\n",
        "        # Convert the grayscale values to binary (0 or 1)\n",
        "        mask_array = (mask_array > 0).astype(np.uint8)\n",
        "\n",
        "        label_area = np.sum(mask_array == 1)\n",
        "        logger.info(f\"Label area : {label_area} \\t Image area : {img_area}\")\n",
        "\n",
        "        # Optionally, you can display the NumPy array\n",
        "        logger.info(f'Label {file_name} \\t {mask_array.shape}')\n",
        "        false_positives = []\n",
        "        max_mask_size, max_mask_id, max_iou, max_rec, max_pre, max_f1 = 0,0,0,0,0,0\n",
        "        ious, recs, pres, f1s = [], [], [], []\n",
        "        for i,mask in enumerate(masks):\n",
        "            if mask[\"status\"] == \"ok\":\n",
        "                mask_pred = mask[\"segmentation\"]\n",
        "                mask_area = mask[\"area\"]\n",
        "                # Convert the grayscale values to binary (0 or 1)\n",
        "                mask_pred = (mask_pred > 0).astype(np.uint8)\n",
        "                if  exceeds_iou_threshold(mask_pred,mask_array, IOU_THRESH): #intersects(mask_pred,mask_array):\n",
        "                    #Convert to boundary mask and recompute metrics if dilation < 1\n",
        "                    if DILATION < 1:\n",
        "                        mask_array = mask_to_boundary(mask_array,DILATION)\n",
        "                        mask_pred = mask_to_boundary(mask_pred,DILATION)\n",
        "                    #Compute metrics\n",
        "                    iou, rec, pre, f1 = calculate_metrics(mask_pred,mask_array)\n",
        "                    logger.info(f'MASK-{i} iou={round(iou, 4):<20} rec={round(rec, 4):<20} pre={round(pre, 4):<20} f1={round(f1,4):<20} size={max_mask_size}')\n",
        "                    false_positives.append(mask_pred)\n",
        "                    ious.append(iou)\n",
        "                    recs.append(rec)\n",
        "                    pres.append(pre)\n",
        "                    f1s.append(f1)\n",
        "                    if iou > max_iou:\n",
        "                        false_positives.pop()\n",
        "                        max_iou = iou\n",
        "                        max_rec = rec\n",
        "                        max_pre = pre\n",
        "                        max_f1 = f1\n",
        "                        max_mask_id = i\n",
        "                        max_mask_size = mask_area\n",
        "        mean_iou = sum(ious) / len(ious) if ious else 0\n",
        "        mean_rec = sum(recs) / len(recs) if recs else 0\n",
        "        mean_pre = sum(pres) / len(pres) if pres else 0\n",
        "        mean_f1 = sum(f1s) / len(f1s) if f1s else 0\n",
        "\n",
        "        fpr = compute_fpr(false_positives,mask_array)\n",
        "\n",
        "        #logger.info(f'WINNER -> MASK-{max_mask_id} iou = {round(max_iou,4):<20} ')\n",
        "        logger.info(f'MASK-{max_mask_id} m_iou={round(mean_iou, 4):<20} m_rec={round(mean_rec, 4):<20} m_pre={round(mean_pre, 4):<20} m_f1={round(mean_f1,4):<20} label_fpr={round(fpr,4):<20} size={max_mask_size}')\n",
        "        if args.par and mixin_coeef[image_name]:\n",
        "            numerator = mixin_coeef[image_name][file_name]\n",
        "            denominator = sum(mixin_coeef[image_name].values())\n",
        "            print(f'Using mixin coeef: {file_name} = {numerator}/{denominator}')\n",
        "        else:\n",
        "            numerator = 1\n",
        "            denominator = num_gt_labels\n",
        "        avg_iou += (numerator / denominator) * mean_iou * 100\n",
        "        avg_pre += (numerator / denominator) * mean_pre * 100\n",
        "        avg_rec += (numerator / denominator) * mean_rec * 100\n",
        "        avg_f1 += (numerator / denominator) * mean_f1 * 100\n",
        "        total_fpr += (numerator / denominator) * fpr * 100\n",
        "\n",
        "\n",
        "    logger.info(f'SAM Avg IOU                 : {round(avg_iou,2)}%')\n",
        "    logger.info(f'SAM Avg PRECISION           : {round(avg_pre,2)}%')\n",
        "    logger.info(f'SAM Avg RECALL              : {round(avg_rec,2)}%')\n",
        "    logger.info(f'SAM Avg F1 SCORE            : {round(avg_f1,2)}%')\n",
        "    logger.info(f'SAM Avg FALSE POSITIVE RATE : {round(total_fpr,2)}%')\n",
        "\n",
        "    results[IMAGE_ALIAS] = [round(avg_iou,2), round(avg_rec,2), round(avg_pre,2), round(avg_f1,2), round(total_fpr,2)]\n",
        "\n",
        "    plt.figure(figsize=(20,20))\n",
        "    plt.imshow(image)\n",
        "    show_anns([mask for mask in masks if mask[\"status\"] == \"ok\"])\n",
        "    plt.axis('off')\n",
        "    for idx, mask in enumerate(masks):\n",
        "        if mask[\"status\"] == \"ok\":\n",
        "            bbox = (mask[\"bbox\"][0],mask[\"bbox\"][1],mask[\"bbox\"][0] + mask[\"bbox\"][2],mask[\"bbox\"][1] + mask[\"bbox\"][3])\n",
        "            show_box(bbox, plt.gca(),f\"Mask-{idx}\",mask[\"color\"] if \"color\" in mask else [])\n",
        "            show_points(np.array(mask[\"point_coords\"]), np.array([1]), plt.gca())\n",
        "\n",
        "    if args.post:\n",
        "        #Legend\n",
        "        # Create empty handles and labels lists\n",
        "        handles = []\n",
        "        labels = []\n",
        "        # Iterate over the colors and create proxy artists\n",
        "        for i, color in color_code.items():\n",
        "            # Create a rectangle patch as the proxy artist\n",
        "            rect = plt.Rectangle((0, 0), 1, 1, color=color)\n",
        "            handles.append(rect)\n",
        "            labels.append(f'Cluster-{i}')\n",
        "        plt.legend(handles, labels, loc='upper left')\n",
        "\n",
        "    sam_out = log_dir + '_sam.pdf' if args.saveformat == 'pdf' else log_dir + '_sam.png'\n",
        "    plt.draw()\n",
        "    plt.savefig(sam_out, bbox_inches='tight', pad_inches=0, format='pdf' if args.saveformat == 'pdf' else 'png')\n",
        "    plt.close()\n",
        "\n",
        "    for i,_ in enumerate(masks):\n",
        "        if mask[\"status\"] == \"ok\":\n",
        "            plt.figure(figsize=(20,20))\n",
        "            plt.imshow(image)\n",
        "            show_mask(masks[i][\"segmentation\"], plt.gca())\n",
        "            #show_anns(masks)\n",
        "            plt.axis('off')\n",
        "            for chip in masks[i][\"chips\"]:\n",
        "                show_box(chip[1:], plt.gca())\n",
        "            bbox = (masks[i][\"bbox\"][0],masks[i][\"bbox\"][1],masks[i][\"bbox\"][0] + masks[i][\"bbox\"][2],masks[i][\"bbox\"][1] + masks[i][\"bbox\"][3])\n",
        "            show_box(bbox, plt.gca())\n",
        "            show_points(np.array(masks[i][\"point_coords\"]), np.array([1]), plt.gca())\n",
        "            iname = IMAGE_PATH.split(\".\")[-2].split(\"/\")[-1] + \"/\" + f\"MASK-{i}\" + \"/\"\n",
        "            directory_path = os.path.abspath(os.getcwd()) + OUTPUT_DIR + iname\n",
        "            mask_name = f\"MASK-{i}.jpg\"\n",
        "            plt.draw()\n",
        "            if not os.path.exists(directory_path):\n",
        "                os.makedirs(directory_path)\n",
        "            plt.savefig(directory_path + mask_name)\n",
        "            plt.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30LAlPZKYcZ-",
        "outputId": "5dd00e03-d967-407d-99d7-b38b5f16d281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing: ('STEM_JEOL_ADF1_03-16-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_0002.jpg',)\n",
            "IMAGE: STEM_JEOL_ADF1_03-16-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_0002.jpg\n",
            "STEM_JEOL_ADF1_03-16-20_Wangoh_LSTO_-_STO_0-25_TEM_012020_LO_0_031020_0002: clusters  found!\n",
            "Chip size: 60 x 60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Summarize results into its own dictionary\n",
        "results[\"SUMMARY\"] = summarize_results(results)\n",
        "\n",
        "#Write results to file\n",
        "titles = [\"IMAGE\", \"IOU\", \"RECALL\", \"PRECISION\", \"F1\", \"FPR\"]\n",
        "spacing = [100,10,10,10,10,10]\n",
        "write_to_file(results, titles, os.path.abspath(os.getcwd()) + OUTPUT_DIR + 'options.log',spacing)"
      ],
      "metadata": {
        "id": "cN6XoDFnYsVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd SamIAm; git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKBmpQdifNq3",
        "outputId": "799b7a9a-3b28-486d-e4e2-4875bf46eff9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 2, done.\u001b[K\n",
            "remote: Counting objects:  50% (1/2)\u001b[K\rremote: Counting objects: 100% (2/2)\u001b[K\rremote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 2 (delta 1), reused 2 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects:  50% (1/2)\rUnpacking objects: 100% (2/2)\rUnpacking objects: 100% (2/2), 207 bytes | 207.00 KiB/s, done.\n",
            "From https://github.com/PerfLab-EXaCT/SamIAm\n",
            "   9e9d7f9..faf179b  main       -> origin/main\n",
            "Updating 9e9d7f9..faf179b\n",
            "Fast-forward\n",
            " FENet.pth | Bin \u001b[31m0\u001b[m -> \u001b[32m48037071\u001b[m bytes\n",
            " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
            " create mode 100644 FENet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iv_uDqnVfQMB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}